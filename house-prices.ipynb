{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"*https://www.kaggle.com/eddiery/*\n## House Prices - Advanced Regression Techniques | Kaggle Comptetion\n\n### Comptetion Description and challenge\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\nTo solve this challange I'm going to use Linear Regression Models using diffrent type of approaches with Cross Validtation and different Feature Selection Models","metadata":{}},{"cell_type":"markdown","source":"### Imports and Definitions\n","metadata":{}},{"cell_type":"markdown","source":"First we have to update some libraries","metadata":{}},{"cell_type":"code","source":"!pip install -U scikit-learn\n!pip install --upgrade plotly\n!pip uninstall matplotlib -Y\n!pip install matplotlib","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:45:07.937896Z","iopub.execute_input":"2022-02-10T14:45:07.938573Z","iopub.status.idle":"2022-02-10T14:46:29.143026Z","shell.execute_reply.started":"2022-02-10T14:45:07.938415Z","shell.execute_reply":"2022-02-10T14:46:29.142125Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy\nfrom scipy import stats\nfrom scipy.stats import norm\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n\nfrom pandas_profiling import ProfileReport\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# sklearn imports\nimport sklearn\nfrom sklearn import metrics\nfrom sklearn import datasets\nfrom sklearn import pipeline\n\n\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\n\nfrom sklearn import metrics\nfrom sklearn import pipeline\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nfrom sklearn import neural_network\nfrom sklearn import model_selection\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import LeavePOut\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import KFold\nfrom tqdm.auto import tqdm\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-10T14:46:29.145906Z","iopub.execute_input":"2022-02-10T14:46:29.146268Z","iopub.status.idle":"2022-02-10T14:46:33.112441Z","shell.execute_reply.started":"2022-02-10T14:46:29.146222Z","shell.execute_reply":"2022-02-10T14:46:33.111312Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Turn on autocomplete\n%config Completer.use_jedi = False","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:46:33.114199Z","iopub.execute_input":"2022-02-10T14:46:33.114687Z","iopub.status.idle":"2022-02-10T14:46:33.131461Z","shell.execute_reply.started":"2022-02-10T14:46:33.114632Z","shell.execute_reply":"2022-02-10T14:46:33.130449Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data_path = '../input/house-prices-advanced-regression-techniques/train.csv'\ntest_path = '../input/house-prices-advanced-regression-techniques/test.csv'\nsubmission_path = '../input/house-prices-advanced-regression-techniques/sample_submission.csv'\n\ndf_original = pd.read_csv(data_path)\ntest_df = pd.read_csv(test_path)\nsubmission_df = pd.read_csv(submission_path)\ndisplay(df_original)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:46:33.133963Z","iopub.execute_input":"2022-02-10T14:46:33.134996Z","iopub.status.idle":"2022-02-10T14:46:33.273624Z","shell.execute_reply.started":"2022-02-10T14:46:33.134940Z","shell.execute_reply":"2022-02-10T14:46:33.272553Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Data Processing","metadata":{}},{"cell_type":"markdown","source":"Let's have a look on our data, we have 81 different features, we should examine our data and check for missing valuses, and understand which features are the best before we start train models.","metadata":{}},{"cell_type":"code","source":"df_original.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:46:33.275046Z","iopub.execute_input":"2022-02-10T14:46:33.275381Z","iopub.status.idle":"2022-02-10T14:46:33.314719Z","shell.execute_reply.started":"2022-02-10T14:46:33.275341Z","shell.execute_reply":"2022-02-10T14:46:33.313459Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Check for empty values","metadata":{}},{"cell_type":"code","source":"print(\"Check if there are columns with empty values\")\ndisplay(df_original.isna().any())\n\nprint(f'There are {len(np.where(df_original.isnull())[0])} empty values in the dataframe')\n\n# count empty values in each column\ndef count_empty_values_in_each_column(df: pd.DataFrame):\n  print('empty values')\n  print('------------\\n')\n  \n  for col in df.columns:\n    if df[col].isna().sum() > 0:\n        print(f\"{col}: {df[col].isna().sum()}\")\n\ncount_empty_values_in_each_column(df_original)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:46:33.316098Z","iopub.execute_input":"2022-02-10T14:46:33.316356Z","iopub.status.idle":"2022-02-10T14:46:33.388590Z","shell.execute_reply.started":"2022-02-10T14:46:33.316322Z","shell.execute_reply":"2022-02-10T14:46:33.387567Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"We can see that we have some features with missing values. \nFor example MiscFeature has 1406 miss values out of 1460 samples, this is a lot and probably should be dropped.  \nBut before deciding that we should check out corrletion.","metadata":{}},{"cell_type":"markdown","source":"#### Correlation Map","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\ncor = np.abs(df_original.corr())\nsns.heatmap(cor, cmap=plt.cm.Reds, vmin=0, vmax=1, square=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:46:33.390406Z","iopub.execute_input":"2022-02-10T14:46:33.390784Z","iopub.status.idle":"2022-02-10T14:46:34.395521Z","shell.execute_reply.started":"2022-02-10T14:46:33.390731Z","shell.execute_reply":"2022-02-10T14:46:34.394255Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"We can see that most of those features has low correlation with our targer variable - 'SalePrice'.  \nIt probably should be best to drop those features and focus on the highest corrlated features.","metadata":{}},{"cell_type":"code","source":"df_cp = df_original.copy()\ndf_cp","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:46:34.396860Z","iopub.execute_input":"2022-02-10T14:46:34.397722Z","iopub.status.idle":"2022-02-10T14:46:34.433562Z","shell.execute_reply.started":"2022-02-10T14:46:34.397669Z","shell.execute_reply":"2022-02-10T14:46:34.432477Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engeeniring","metadata":{}},{"cell_type":"markdown","source":"### First let's deal with our missing data","metadata":{}},{"cell_type":"code","source":"#missing data\ntotal = df_cp.isnull().sum().sort_values(ascending=False)\npercent = (df_cp.isnull().sum()/df_cp.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:46:34.435159Z","iopub.execute_input":"2022-02-10T14:46:34.435638Z","iopub.status.idle":"2022-02-10T14:46:34.480383Z","shell.execute_reply.started":"2022-02-10T14:46:34.435579Z","shell.execute_reply":"2022-02-10T14:46:34.479650Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"We got a table of all the features with missing data in them.  \nAs I said before this features don't have strong corrleation, and we have a lot of other good featrues with strong correltion so it would be the best to drop them.  \nAbout the one row with 'Elecrtical' missing in it, we can drop the row instead, one row in a quite big dataset won't make a big difference.","metadata":{}},{"cell_type":"code","source":"# dealing with missing data\nprint((missing_data[missing_data['Total'] > 0]).index)\ndf_cp = df_cp.drop((missing_data[missing_data['Total'] > 1]).index,1)\ndf_cp = df_cp.drop(df_cp.loc[df_cp['Electrical'].isnull()].index)\ndf_cp = df_cp.drop(['Id'], axis=1)\ndf_cp.reset_index(drop=True, inplace=True)\ndf_cp","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:46:34.483253Z","iopub.execute_input":"2022-02-10T14:46:34.483714Z","iopub.status.idle":"2022-02-10T14:46:34.527403Z","shell.execute_reply.started":"2022-02-10T14:46:34.483677Z","shell.execute_reply":"2022-02-10T14:46:34.526539Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#check that we indeed have no missing values left\ncount_empty_values_in_each_column(df_cp)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:46:34.528889Z","iopub.execute_input":"2022-02-10T14:46:34.529369Z","iopub.status.idle":"2022-02-10T14:46:34.555891Z","shell.execute_reply.started":"2022-02-10T14:46:34.529314Z","shell.execute_reply":"2022-02-10T14:46:34.554862Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Target variable distribution","metadata":{}},{"cell_type":"code","source":"#histogram and normal probability plot\nsns.distplot(df_cp['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_cp['SalePrice'], plot=plt)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:46:34.557528Z","iopub.execute_input":"2022-02-10T14:46:34.558055Z","iopub.status.idle":"2022-02-10T14:46:35.147633Z","shell.execute_reply.started":"2022-02-10T14:46:34.558004Z","shell.execute_reply":"2022-02-10T14:46:35.146782Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# df_cp['SalePrice'] = np.log(df_cp['SalePrice'])","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:46:35.149301Z","iopub.execute_input":"2022-02-10T14:46:35.150003Z","iopub.status.idle":"2022-02-10T14:46:35.154486Z","shell.execute_reply.started":"2022-02-10T14:46:35.149952Z","shell.execute_reply":"2022-02-10T14:46:35.153547Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Before we start creating new features let's have a look on the top most correlated features.","metadata":{}},{"cell_type":"code","source":"#saleprice correlation matrix\nplt.figure(figsize=(12,10))\nk = 10 #number of variables for heatmap\ncols = cor.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_cp[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, fmt='.2f', yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:46:35.156198Z","iopub.execute_input":"2022-02-10T14:46:35.156526Z","iopub.status.idle":"2022-02-10T14:46:35.929904Z","shell.execute_reply.started":"2022-02-10T14:46:35.156462Z","shell.execute_reply":"2022-02-10T14:46:35.929238Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"We can see now that TotalBsmtSF and 1stFlrSF have strong corrleation.\nTotalBsmtSF: Total square feet of basement area.  \n1stFlrSF: First Floor square feet.  \nThese features are connected and we know we have another feature that simmilart to them which is 2ndFlrSF.  \n2ndFlrSF: Second floor square feet.  \nThese 3 features can be combined and give us a new feature - Total area of the house.  \nAlso we have YearBuild, which is also have simmilar feature - YrSold. \nWith both of these feature we can get the age of the house when it was sold: YrSold - YearBuild\n\n","metadata":{}},{"cell_type":"markdown","source":"#### Creating New Features","metadata":{}},{"cell_type":"code","source":"df_train = df_cp.copy()\ndf_train['TotalSF'] = df_train['TotalBsmtSF'] + df_train['1stFlrSF'] + df_train['2ndFlrSF']\ndf_train['HouseAge'] = df_train['YrSold'] - df_train['YearBuilt']","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:46:35.931118Z","iopub.execute_input":"2022-02-10T14:46:35.931774Z","iopub.status.idle":"2022-02-10T14:46:35.938892Z","shell.execute_reply.started":"2022-02-10T14:46:35.931734Z","shell.execute_reply":"2022-02-10T14:46:35.938211Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot('TotalSF', 'SalePrice', data=df_train, palette='Set2', color=\"teal\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:46:35.939995Z","iopub.execute_input":"2022-02-10T14:46:35.940534Z","iopub.status.idle":"2022-02-10T14:46:36.226835Z","shell.execute_reply.started":"2022-02-10T14:46:35.940497Z","shell.execute_reply":"2022-02-10T14:46:36.225749Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot('HouseAge', 'SalePrice', data=df_train, palette='Set2', color=\"teal\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:46:36.228419Z","iopub.execute_input":"2022-02-10T14:46:36.228732Z","iopub.status.idle":"2022-02-10T14:46:36.800135Z","shell.execute_reply.started":"2022-02-10T14:46:36.228697Z","shell.execute_reply":"2022-02-10T14:46:36.799231Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"This new features indeed has a good corolletion with the targert variable  \nWe can learn that as long as the house age is lower the price is higher  \nAlso as long as the total surface is higher the price is higher.  \nThese two conclusions make sense.","metadata":{}},{"cell_type":"markdown","source":"### Cross-Validation\nFor spliting our data into train and valiation we can  \nFor spliting our data into train and valiation we can  use CV (Cross-Validation) instead of breaking the data to train-validation splits.  \nWhen we use CV, we have better predictions of the test results.  \nIt is similar to splitting the data, but we make sure that our split won't affect our result (we try a few possible splits).  \nWe can use 2 CV methods:  \n      1. KFold  \n      2.LPO (Leave P out)  \nWe will use KFold when we want speed.  \nWe will use LPO when we want to be more precise and better predict the test score.  ","metadata":{}},{"cell_type":"code","source":"# divide the data to features and target\nt = df_train['SalePrice'].copy()\nX = df_train.drop(['SalePrice'], axis=1)\nprint('t')\ndisplay(t)\nprint()\nprint('X')\ndisplay(X)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:50:41.459014Z","iopub.execute_input":"2022-02-10T14:50:41.459297Z","iopub.status.idle":"2022-02-10T14:50:41.515541Z","shell.execute_reply.started":"2022-02-10T14:50:41.459260Z","shell.execute_reply":"2022-02-10T14:50:41.513556Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:50:41.517005Z","iopub.execute_input":"2022-02-10T14:50:41.517302Z","iopub.status.idle":"2022-02-10T14:50:41.523745Z","shell.execute_reply.started":"2022-02-10T14:50:41.517265Z","shell.execute_reply":"2022-02-10T14:50:41.523023Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def find_generator_len(generator, use_pbar=True):\n    i = 0\n    \n    if use_pbar:\n        pbar = tqdm(desc='Calculating Length', ncols=1000, bar_format='{desc}{bar:10}{r_bar}')\n\n    for a in generator:\n        i += 1\n\n        if use_pbar:\n            pbar.update()\n\n    if use_pbar:\n        pbar.close()\n\n    return i","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:50:41.524893Z","iopub.execute_input":"2022-02-10T14:50:41.525668Z","iopub.status.idle":"2022-02-10T14:50:41.563049Z","shell.execute_reply.started":"2022-02-10T14:50:41.525626Z","shell.execute_reply":"2022-02-10T14:50:41.561564Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Let's create a method that gets: data and model and returns R2 score and MSE loss","metadata":{}},{"cell_type":"code","source":"# calculate score and loss from cv (KFold or LPO) and display graphs\ndef get_cv_score_and_loss(X, t, model, k=None, p=None, show_score_loss_graphs=False, use_pbar=True):\n    scores_losses_df = pd.DataFrame(columns=['fold_id', 'split', 'score', 'loss'])\n\n    if k is not None:\n        cv = KFold(n_splits=k, shuffle=True, random_state=1)\n    elif p is not None:\n        cv = LeavePOut(p)\n    else:\n        raise ValueError('you need to specify k or p in order for the cv to work')\n\n    if use_pbar:\n        pbar = tqdm(desc='Computing Models', total=find_generator_len(cv.split(X)))\n\n    for i, (train_ids, val_ids) in enumerate(cv.split(X)):\n        X_train = X.loc[train_ids]\n        t_train = t.loc[train_ids]\n        X_val = X.loc[val_ids]\n        t_val = t.loc[val_ids]\n\n        model.fit(X_train, t_train)\n\n        y_train = model.predict(X_train)\n        y_val = model.predict(X_val)\n        scores_losses_df.loc[len(scores_losses_df)] = [i, 'train', model.score(X_train, t_train), mean_squared_error(t_train, y_train, squared=False)]\n        scores_losses_df.loc[len(scores_losses_df)] = [i, 'val', model.score(X_val, t_val), mean_squared_error(t_val, y_val, squared=False)]\n\n        if use_pbar:\n            pbar.update()\n\n    if use_pbar:\n        pbar.close()\n\n    val_scores_losses_df = scores_losses_df[scores_losses_df['split']=='val']\n    train_scores_losses_df = scores_losses_df[scores_losses_df['split']=='train']\n\n    mean_val_score = val_scores_losses_df['score'].mean()\n    mean_val_loss = val_scores_losses_df['loss'].mean()\n    mean_train_score = train_scores_losses_df['score'].mean()\n    mean_train_loss = train_scores_losses_df['loss'].mean()\n\n    if show_score_loss_graphs:\n        fig = px.line(scores_losses_df, x='fold_id', y='score', color='split', title=f'Mean Val Score: {mean_val_score:.2f}, Mean Train Score: {mean_train_score:.2f}')\n        fig.show()\n        fig = px.line(scores_losses_df, x='fold_id', y='loss', color='split', title=f'Mean Val Loss: {mean_val_loss:.2f}, Mean Train Loss: {mean_train_loss:.2f}')\n        fig.show()\n\n    return mean_val_score, mean_val_loss, mean_train_score, mean_train_loss","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:50:41.565361Z","iopub.execute_input":"2022-02-10T14:50:41.566251Z","iopub.status.idle":"2022-02-10T14:50:41.593423Z","shell.execute_reply.started":"2022-02-10T14:50:41.566178Z","shell.execute_reply":"2022-02-10T14:50:41.592599Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# determine categorical and numerical features\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\ncategorical_cols = X.select_dtypes(include=['object', 'bool']).columns\nall_cols = np.array(X.columns)\n\nprint('Numerical Cols:', numerical_cols)\nprint('Categorical Cols:', categorical_cols)\nprint('All Cols:', all_cols)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:50:41.594916Z","iopub.execute_input":"2022-02-10T14:50:41.595325Z","iopub.status.idle":"2022-02-10T14:50:41.619079Z","shell.execute_reply.started":"2022-02-10T14:50:41.595280Z","shell.execute_reply":"2022-02-10T14:50:41.617866Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# use column transformer to insert different transformers for each column\nfrom sklearn.compose import ColumnTransformer\n\nct = ColumnTransformer([\n    (\"encoding\", OneHotEncoder(sparse=False, handle_unknown='ignore'), categorical_cols),\n    (\"standard\", StandardScaler(), numerical_cols)])\nmodel_pipe = make_pipeline(ct, SGDRegressor(random_state=1))\nval_score, val_loss, train_score, train_loss = get_cv_score_and_loss(X, t, model_pipe, k=10, show_score_loss_graphs=True)\nprint(f'mean cv val score: {val_score:.2f}\\nmean cv val loss {val_loss:.2f}')\nprint(f'mean cv val score: {train_score:.2f}\\nmean cv val loss {train_loss:.2f}')","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:50:41.620786Z","iopub.execute_input":"2022-02-10T14:50:41.621331Z","iopub.status.idle":"2022-02-10T14:50:44.516832Z","shell.execute_reply.started":"2022-02-10T14:50:41.621294Z","shell.execute_reply":"2022-02-10T14:50:44.515907Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Selection\n\nWe want to choose the best features for our use case.  \nWe have learned 3 methods of Feature Selection:  \n1. Forward Feature Selection  \n2. Backward Feature Selection  \n3. Hybrid Feature Selection     \nIn Forward Feature Selection we start from zero features and add features until we reach the number of maximum features or until we reach the best score.  \nIn Backward Feature Selection we start from the full feature set and remove features until we reach the number of minimum features or until we reach the best score.  \nIn Hybrid Feature Selection, we start from zero features and add/remove features until we reach the best score.  \n  \nWe will use Scikit-learn RFE that is based on the Backward Feature Selection.  \nWe will choose the best features of this dataset with this method.  ","metadata":{}},{"cell_type":"code","source":"#  choose the best features of this dataset with SGDRegressor\n\nct_encoded = ColumnTransformer([\n                (\"encoding\", OrdinalEncoder(), categorical_cols),\n                (\"standard\", StandardScaler(), numerical_cols)])\nX_encoded = pd.DataFrame(ct_encoded.fit_transform(X, t), columns=all_cols)\n\nselector = RFECV(SGDRegressor(random_state=1), cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=1)).fit(X_encoded, t)\ntop_features_df = X_encoded.loc[:, selector.support_]\nnum_features = len(selector.support_)\nprint('_______', num_features, ' most segnificant features are:_______')\ndisplay(top_features_df.columns)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:50:44.518306Z","iopub.execute_input":"2022-02-10T14:50:44.518569Z","iopub.status.idle":"2022-02-10T14:52:51.331361Z","shell.execute_reply.started":"2022-02-10T14:50:44.518537Z","shell.execute_reply":"2022-02-10T14:52:51.330119Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# best_features = ['MSZoning', 'Street', 'LandSlope', 'Condition1', 'Condition2',\n#        'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation',\n#        'Heating', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional',\n#        'PavedDrive', 'SaleType', 'SaleCondition', 'LotArea', 'OverallCond',\n#        'YearBuilt', 'YearRemodAdd', 'BsmtFinSF2', '1stFlrSF', '2ndFlrSF',\n#        'LowQualFinSF', 'GrLivArea', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr',\n#        'GarageArea', 'HouseAge']\nbest_features = top_features_df.columns.tolist()\nprint(best_features)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:52:51.334921Z","iopub.execute_input":"2022-02-10T14:52:51.335576Z","iopub.status.idle":"2022-02-10T14:52:51.343428Z","shell.execute_reply.started":"2022-02-10T14:52:51.335523Z","shell.execute_reply":"2022-02-10T14:52:51.341876Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"#### Testing the best features we got","metadata":{}},{"cell_type":"code","source":"X_test_temp = df_train.copy()\nX_test_temp.drop('SalePrice',axis=1)\n\nX_test_temp = df_train[best_features]\n\nt_train_temp = df_train['SalePrice']\n\n    \nbest_numerical_cols = X_test_temp.select_dtypes(include=['int64', 'float64']).columns\nbest_categorical_cols = X_test_temp.select_dtypes(include=['object', 'bool']).columns\nbest_all_cols = np.array(X_test_temp.columns)\n\n\nct = ColumnTransformer([\n    (\"encoding\", OneHotEncoder(sparse=False, handle_unknown='ignore'), best_categorical_cols),\n    (\"standard\", StandardScaler(), best_numerical_cols)])\n\nmodel_pipe = make_pipeline(ct, SGDRegressor(random_state=1))\n\nval_score, val_loss, train_score, train_loss = get_cv_score_and_loss(X_test_temp, t_train_temp, model_pipe, k=10, show_score_loss_graphs=True)\nX_test_temp","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:52:51.345020Z","iopub.execute_input":"2022-02-10T14:52:51.345233Z","iopub.status.idle":"2022-02-10T14:52:53.611509Z","shell.execute_reply.started":"2022-02-10T14:52:51.345207Z","shell.execute_reply":"2022-02-10T14:52:53.610509Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"#### Prediction on train  \nAfter we trained and tested the validation data, we need to test on the original training data before we test the submission.","metadata":{}},{"cell_type":"code","source":"SGD_reg_original = pipeline.make_pipeline(ct, linear_model.SGDRegressor(random_state=1)).fit(X_test_temp, t_train_temp)\n\ny_train = SGD_reg_original.predict(X_test_temp)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:52:53.617710Z","iopub.execute_input":"2022-02-10T14:52:53.617988Z","iopub.status.idle":"2022-02-10T14:52:53.691272Z","shell.execute_reply.started":"2022-02-10T14:52:53.617958Z","shell.execute_reply":"2022-02-10T14:52:53.690025Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"print('Accuracy score on train', SGD_reg_original.score(X_test_temp, t_train_temp))\nprint('RMSE score on train',(mean_squared_error(t_train_temp, y_train, squared=False)))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:52:53.693064Z","iopub.execute_input":"2022-02-10T14:52:53.694171Z","iopub.status.idle":"2022-02-10T14:52:53.734251Z","shell.execute_reply.started":"2022-02-10T14:52:53.694111Z","shell.execute_reply":"2022-02-10T14:52:53.733270Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"We are getting nice results in with this model.  \nWe can try and check if we can improve it with Regultiazations and Hyper Paramter Serach.","metadata":{}},{"cell_type":"markdown","source":"#### Regularizations\nWhen we have a case of High-Variance, we can use Regularizations, to reduce it.  \nWith regularization, we can control the size of the model weights and thus control the variance.  \nWe control the size of the model weights by adding a term to the loss function.  \nThis term is called a penalty to high weights.  \nWe have learned about three regularization techniques:  \nL1 (also called Lasso).  \nL2 (also called Ridge).  \nElastic Net (a combination of Lasso and Ridge).  ","metadata":{}},{"cell_type":"code","source":"# print lasso, ridge and elasticnet scores as regression \nfrom sklearn.model_selection import cross_val_score\n\nsgd_lasso_reg = SGDRegressor(penalty='l1', random_state=1)\nsgd_ridge_reg = SGDRegressor(penalty='l2', random_state=1)\nsgd_elastic_reg = SGDRegressor(penalty='elasticnet', random_state=1)\n# X_encoded = standartize_and_dummie_encoding(X_best)\n\nprint(\"R2 score for regression:\")\nprint('sgd_lasso', cross_val_score(make_pipeline(ct, sgd_lasso_reg), X_test_temp, t_train_temp, cv=10).mean())\nprint('sgd_ridge', cross_val_score(make_pipeline(ct, sgd_ridge_reg), X_test_temp, t_train_temp, cv=10).mean())\nprint('sgd_elastic', cross_val_score(make_pipeline(ct, sgd_elastic_reg), X_test_temp, t_train_temp, cv=10).mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:52:53.736421Z","iopub.execute_input":"2022-02-10T14:52:53.737212Z","iopub.status.idle":"2022-02-10T14:52:58.168751Z","shell.execute_reply.started":"2022-02-10T14:52:53.737131Z","shell.execute_reply":"2022-02-10T14:52:58.167716Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"These methods gives us less accurate results than before","metadata":{}},{"cell_type":"code","source":"numerical_cols = X_test_temp.select_dtypes(include=['int64', 'float64']).columns\ncategorical_cols = X_test_temp.select_dtypes(include=['object', 'bool']).columns\nall_cols = np.array(X_test_temp.columns)\n\n\nct = ColumnTransformer([\n    (\"encoding\", OneHotEncoder(sparse=False, handle_unknown='ignore'), categorical_cols),\n    (\"standard\", StandardScaler(), numerical_cols)])","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:52:58.171019Z","iopub.execute_input":"2022-02-10T14:52:58.171786Z","iopub.status.idle":"2022-02-10T14:52:58.185818Z","shell.execute_reply.started":"2022-02-10T14:52:58.171728Z","shell.execute_reply":"2022-02-10T14:52:58.184762Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"X_test_temp_cp = X_test_temp.copy()\nX_test_temp_cp.reset_index(drop=True, inplace=True)\nX_test_temp_cp\n","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:52:58.187529Z","iopub.execute_input":"2022-02-10T14:52:58.188082Z","iopub.status.idle":"2022-02-10T14:52:58.242540Z","shell.execute_reply.started":"2022-02-10T14:52:58.188035Z","shell.execute_reply":"2022-02-10T14:52:58.241649Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"#### Hyper Paramter Search\nMost of our models have a lot of parameters that can be adjusted.  \nEach parameter value can make our model better (or worse).  \nWe want to be able to find the best hyperparameters for our models.  \nWe have two approaches:  \nGrid Search   \nRandom Search  \n  \nGrid Search  \nWhen we want to check every parameter possible, we will use Grid Search.  \nWe will try all combinations of parameters and find the best one, that gives us the best score.  \nThis may be a little exhaustive, especially when we want to check a lot of parameters and values.  \n  \nRandom Search   \nWe can choose to get random combinations of parameters and check the score on them.  \nThis will not be as accurate as Grid Search, but it will take less time.  ","metadata":{}},{"cell_type":"code","source":"# train with grid search and get best parameters\nfrom sklearn.model_selection import GridSearchCV\n\nfeature_arr = ct.fit_transform(X_test_temp, t_train_temp)\nfeature_labels = ct.get_feature_names_out(all_cols.tolist())\nX_encoded = pd.DataFrame(feature_arr, columns=feature_labels)\n\nhyper_parameters = {'penalty': ('l2', 'l1', 'elasticnet'), 'alpha':[0.0001, 0.001, 0.01, 0.1]}\n\ngs_model = GridSearchCV(SGDRegressor(random_state=1), hyper_parameters).fit(X_encoded, t_train_temp)\nprint('Accuracy score for regression:')\nprint('gs_model', gs_model.best_score_)\nprint('best params', gs_model.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:52:58.244269Z","iopub.execute_input":"2022-02-10T14:52:58.244816Z","iopub.status.idle":"2022-02-10T14:53:04.164368Z","shell.execute_reply.started":"2022-02-10T14:52:58.244769Z","shell.execute_reply":"2022-02-10T14:53:04.159666Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# train with random search and get best parameters\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\n\nnp.random.seed(1)\ndistributions = dict(alpha=uniform(loc=0, scale=1), penalty=['l2', 'l1', 'elasticnet'])\n\nrs_model = RandomizedSearchCV(SGDRegressor(random_state=1), distributions, random_state=1).fit(X_encoded, t)\nprint('Accuracy score for regression:')\nprint('rs_model', rs_model.best_score_)\nprint('best params', rs_model.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:53:04.166895Z","iopub.execute_input":"2022-02-10T14:53:04.167507Z","iopub.status.idle":"2022-02-10T14:53:07.656180Z","shell.execute_reply.started":"2022-02-10T14:53:04.167441Z","shell.execute_reply":"2022-02-10T14:53:07.655269Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"With hyper paramter search we see we get a lower score than previouts methods, so in my opinion it's better not be considered.","metadata":{}},{"cell_type":"markdown","source":"### Test  \nNow let's our test for prediction.","metadata":{}},{"cell_type":"code","source":"# func for filling a null column value\ndef fill_random_column(df, column_name):\n    df_not_null = df[~df[column_name].isnull()]\n    df_null = df[df[column_name].isnull()]\n    df[column_name] = df[column_name].apply(lambda x: np.random.choice(df_not_null[column_name]) if pd.isnull(x) else x)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:53:07.658065Z","iopub.execute_input":"2022-02-10T14:53:07.658684Z","iopub.status.idle":"2022-02-10T14:53:07.666446Z","shell.execute_reply.started":"2022-02-10T14:53:07.658639Z","shell.execute_reply":"2022-02-10T14:53:07.665527Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"Creating our new features in the test","metadata":{}},{"cell_type":"code","source":"test_df_feat = test_df.copy()\ntest_df_feat = test_df_feat.drop(['Id'], axis=1)\ntest_df_feat['TotalSF'] = test_df_feat['TotalBsmtSF'] + test_df_feat['1stFlrSF'] + test_df_feat['2ndFlrSF']\ntest_df_feat['HouseAge'] = test_df_feat['YrSold'] - test_df_feat['YearBuilt']\ntest_df_feat.reset_index(drop=True, inplace=True)\ntest_cp_df = test_df_feat[best_features]\n\n\ncount_empty_values_in_each_column(test_cp_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:53:07.668815Z","iopub.execute_input":"2022-02-10T14:53:07.669541Z","iopub.status.idle":"2022-02-10T14:53:07.700883Z","shell.execute_reply.started":"2022-02-10T14:53:07.669462Z","shell.execute_reply":"2022-02-10T14:53:07.699890Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"Fill empty values in the test set","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:06:17.185509Z","iopub.execute_input":"2021-12-14T11:06:17.185814Z","iopub.status.idle":"2021-12-14T11:06:17.192236Z","shell.execute_reply.started":"2021-12-14T11:06:17.185782Z","shell.execute_reply":"2021-12-14T11:06:17.190874Z"}}},{"cell_type":"code","source":"fill_random_column(test_cp_df,'MSZoning')\nfill_random_column(test_cp_df,'Utilities')\nfill_random_column(test_cp_df,'BsmtFinSF1')\nfill_random_column(test_cp_df,'BsmtFinSF2')\nfill_random_column(test_cp_df,'BsmtHalfBath')\nfill_random_column(test_cp_df,'Functional')\nfill_random_column(test_cp_df,'Exterior2nd')\nfill_random_column(test_cp_df,'BsmtUnfSF')\nfill_random_column(test_cp_df,'GarageCars')\n\n\ntest_cp_df","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:53:07.702095Z","iopub.execute_input":"2022-02-10T14:53:07.702323Z","iopub.status.idle":"2022-02-10T14:53:07.775399Z","shell.execute_reply.started":"2022-02-10T14:53:07.702291Z","shell.execute_reply":"2022-02-10T14:53:07.774524Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"count_empty_values_in_each_column(test_cp_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:53:07.776918Z","iopub.execute_input":"2022-02-10T14:53:07.777198Z","iopub.status.idle":"2022-02-10T14:53:07.793505Z","shell.execute_reply.started":"2022-02-10T14:53:07.777147Z","shell.execute_reply":"2022-02-10T14:53:07.792444Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"#### PREDICTION","metadata":{}},{"cell_type":"code","source":"final_predict = SGD_reg_original.predict(test_cp_df)\nfinal_predict","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:53:07.794849Z","iopub.execute_input":"2022-02-10T14:53:07.795098Z","iopub.status.idle":"2022-02-10T14:53:07.829855Z","shell.execute_reply.started":"2022-02-10T14:53:07.795068Z","shell.execute_reply":"2022-02-10T14:53:07.828229Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"#### SUBMISSION","metadata":{}},{"cell_type":"code","source":"submission_df['SalePrice'] = final_predict\nsubmission_df.to_csv('submission.csv', index = False)\nprint(\"Submited\")","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:53:07.830990Z","iopub.execute_input":"2022-02-10T14:53:07.831210Z","iopub.status.idle":"2022-02-10T14:53:07.848880Z","shell.execute_reply.started":"2022-02-10T14:53:07.831183Z","shell.execute_reply":"2022-02-10T14:53:07.848271Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"#### Submissions and Ledaerboard  \n\nThe best score I got is 0.17. \nIn this work i learned a lot about feature selection and encoidng.  \nI struggled with encoding methods, it took me some time to figure out what is the best way to make it work as it should and fit the models I'm creating.","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:06:17.203798Z","iopub.status.idle":"2021-12-14T11:06:17.204669Z","shell.execute_reply.started":"2021-12-14T11:06:17.20433Z","shell.execute_reply":"2021-12-14T11:06:17.204363Z"}}}]}